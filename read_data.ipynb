{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "from data_loader import get_dataset_df\n",
    "from run_sa import get_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from SA.args import get_model_args\n",
    "from baselines import aggregate_print_rouges\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from nltk.parse.corenlp import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser('http://localhost:9000', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(C=300, algo_type='noSA', batch_size=1, dataset_name='pubhealth', dataset_path='/Users/jolly/PycharmProjects/COPENLU/liar_data/ruling_oracles_val.tsv', delete_th=0.97, device_type='cpu', editor_model_id='roberta-base', fluency_weight=1.4, fluencyscorer_model_id='gpt2', insert_th=1.1, length_weight=1.25, max_steps=200, min_length_of_edited_sent=40, named_entity_score_weight=0.95, outdir='', outfile='', outfile_filtered='', outfile_pegasus='', pegasus_modelname='tuner007/pegasus_paraphrase', reorder_th=0.95, sample=None, sbertname_pegasus='paraphrase-distilroberta-base-v1', seed=33, semantic_weight_keywords=1.0, semantic_weight_sentences=1.1, sentences_path='/Users/jolly/PycharmProjects/COPENLU/results_serialized_val_filtered.jsonl', split='test', t_init=60000, top_n=6)\n",
      "Size of dataset: 1248\n",
      "Sample:  {'claim_id': '11972.json', 'statement': 'Building a wall on the U.S.-Mexico border will take literally years.', 'justification': 'Perry said: \"Building a wall\" on the U.S.-Mexico border \"will take literally years.\" If Trump has a fast-track plan to plan the wall, purchase required land, complete needed studies and erect the wall in a year or less, it’s not public. Meantime, engineering experts agree the wall would most likely take years to complete. Keep in mind, too, it took more than six years to build roughly 700 miles of fence and barriers along the roughly 2,000-mile U.S.-Mexico border. Click here formore on the six PolitiFact ratings and how we select facts to check.', 'ruling_without_summary': 'Donald Trump says part of the answer to the complicated question of immigration has a seemingly simple solution: Build a \"great, great\" wall along the southern border and make neighboring Mexico pay for it. The logistics of that idea leave many people, including former Texas Gov. Rick Perry, agog. \"Building a wall\" on the border \"will take literally years. I don’t care how good of a builder you are,\" Perry said about 40 minutes into an interview on the Jan. 15, 2016, edition of the Simon Conway Show on WHO in Des Moines. \"And plus the private property rights,\" Perry said. \"This is an issue that really gets complex in a hurry. I know because I’ve had to deal with it.\" Perry’s assertion got us thinking, what would go into building a wall from southernmost California to the southern tip of Texas? And would Trump’s wall indeedtake years to build? We didn’t take Perry’s claim as an uncheckable predictionin that when he spoke, there already existed exhaustive studies and reports onbuilding a border fence as ordered by Congress in 2006, particularly those conducted by the General Accounting Office, the nonpartisan investigative arm of Congress. We failed to connect with Trump’s camp about this statement while a Perry spokesman, Stan Gerdes, said by phone that while the former governor didn’t send a crew to the border to estimate how long a wall would take to build, it’s Perry’s experience-rich opinion it would definitely take more than a year. Some assumptions Before assessing Perry’s claim, we have to make assumptionsin part because Trump’s plan for the wall is pretty vague. We’re also going to share declarations by Trump and others that occurred after Perry spoke. Typically, we wouldn’t do so out of fairness to the person making a claim. But not doing so here might leave us short of being up to date. What we know of late is that Trumptold MSNBC on Feb. 9, 2016 the wall would be a \"real wall,\" likely 35 to 40 feet tall and 1,000 miles long. The U.S.-Mexico border is actually just less than 2,000 miles long, but Trump said there are natural barriers over a portion of the border, meaning the U.S. would only need a wall along 1,000 miles of it. Some perspective: A 1,000-mile wall would be about one-13th the length of the Great Wall of China, butmore than twice the proposed length of the concrete and fence barrier that separates Israel from the West Bank, according to a Nov. 11, 2014 Washington Post news article. Trump has also conceded the wall will cost billions. He’s been quoted saying costs will total up to about $8 billion, all out of Mexico’s pocketbook. At the Feb. 25, 2016, GOP debate, he upped that speculated price tag to $10 billion to $12 billion. Former Mexican President Felipe Calderón opposes Trump’s idea, according to a Feb. 8, 2016 CNN news article, where Calderon called Trump’s wall \"stupid,\" as does former Mexican President Vicente Fox, who said in a Univision interview posted online Feb. 25, 2016 that Mexico is \"not going to pay for that f****** wall.\" Trump’s response to Fox’s declaration? \"The wall just got 10 feet taller,\" he said at the Republican presidential debate in Houston the same day. In August 2015, Trump told New Hampshire residents the wall would be made using \"beautiful nice precast plank\" (see the video above this story). So we know it’s going to be a solid wall. And if Trump becomes president, is it feasible to have a wall up in a year or less? The existing fence There’s currently about 670 miles of fencing along the 1,954-mile U.S.-Mexico border. That is, the fence is not a fluid structure from point A to point B. Rather, it’s a fragmented barrier of metal posts and fencing standing about 18-feet tall, according to a Jan. 1, 2016, Associated Press news article. When he was governor and running for president the first time, Perry voiced opposition to the fence, expressing a sentiment perhaps echoed in his skepticism about Trump’s promised wall. \"No, I don’t support a fence on the border,\" Perry said in New Hampshire, according to a Sept. 3, 2011 news story in the Dallas Morning News. \"The fact is, it’s 1,200 miles from Brownsville to El Paso. Two things: How long you think it would take to build that? And then if you build a 30-foot wall from El Paso to Brownsville, the 35-foot ladder business gets real good.\" Earlier, the fence was started as a result of the 2006 Secure Fence Act, which approved the construction of an about 700-mile fence stretching from parts of Tecate, California to Brownsville. Five years later, President Barack Obama said the fence is \"now basically complete\" – a claim PolitiFact deemed Mostly False on May 16, 2011. Around that time, the vast majority of the mandated fencing had been answered with vehicle barriers and single-layer pedestrian fence even while the act called for double-layer fencing, of which there was about 36 miles in place. As of 2012, the fence and fence-related costs totaled about $6 billion. A 2009 Government Accountability Office report said that on average, the cost had run between $1 million and $3.9 million per mile of fence, depending on things like \"type of fencing, topography, materials used, land acquisition costs, and labor costs.\" Raul Meza, a structural engineer and El Paso’s state director for the Structural Engineer’s Association of Texas, who lives about four miles from a portion of the border fence, told us by phone he thinks a wall would be even more expensive to build because it requires more time and labor. In January 2016, PolitiFact Florida found estimates for the wall’s cost ranging from $5.1 billion to $25 billion, with additional costs for maintenance. There’s been little announced in terms of a timeline for the wall’s construction. Trump told Meet the Press on Aug. 15, 2016 construction would be \"under budget and ahead of schedule.\" Constructing Trump’s wall By telephone, Sharon Wood, dean of the University of Texas’ Cockrell School of Engineering and a former chair of UT’s civil engineering department, told us a wall,like the one Trump is suggesting,would likely be made using steel-reinforced concrete. \"You can buy small reinforcement bars if you just go to Home Depot… Obviously for a larger structure, you’re going to use larger sizes,\" Wood said. \"You don\\'t buy it from Home Depot, but it’s the same type of process.\" A Feb. 17, 2016, CNN news article backs up this idea. Engineers asked to speculate about the envisioned project suggested precast concrete panels reinforced with steel would likely be used.They also came to the conclusion that building the wall would take around 339 million cubic feet of concrete and 5 billion pounds of steel. Meza told us two ways to build a reinforced concrete wall would entail either building it along the border on site, or building concrete panels at different construction firms -- which Trump seemed to endorse in his August 2015 remarks -- and having the panels transported and installed.Meza said precast panels would save time but add to the bill due to transport costs. Yet there’s more to building a large project than outright construction. James Jirsa, a UT-Austin civil engineering professor who specializes in concrete structures, told us there’s usually a planning period, lasting at least a year, to survey terrain and settle design issues. That period, he said, is followed by a bidding period for land that can also be time-consuming. The federal government has the right to build on private property when it deems projects a public necessity, though it is required to offer compensation – a process, celebrated by Trump, called eminent domain. This part of pre-construction can take years, depending on whether or not landowners dispute it. \"Every piece of land is different,\" said Paul Barkhurst, an eminent domain litigation lawyer based in San Antonio. \"You’re talking about a massive project across many, many states. It just depends on how much resources they want to put on it. One case could drag on for years because the land owner can challenge the right to take and the landowner can challenge the amount of compensation.\" Wood said an environmental impact and hydrological study may also be necessary, to see how the wall would affect nearby water and the flow in the Rio Grande. Then comes the raw building of the wall. Wood said there aren’t many structural loads to consider, but the builders would have to find a way to make sure the foundation is solid, even if the Rio Grande floods. This involves digging out the foundation and putting in casts of the concrete foundation and then building. \"To be honest, that’s pretty straightforward,\" Wood said,saying the excavation of the foundation would probably take the most time. So how long would it take? Wood said that when just looking at the building the wall, pre-construction steps aside, it’s possible though highly unlikely that the wall could be completed in a year, say, if it were a\"national priority and all of the resources were put in one place.\" If \"you were to mobilize every single construction worker in the country,\" Wood said, \"and take them down to the Texas border, I bet they could get it done very quickly.\" Separately, Jirsa opined that he and most engineers would probably agree that such a wall could not be built in under a year while Meza said the time it takes to build the avowed wall hinges largely on how much money goes toward construction. If money were no object, he said, the best-case scenario from the initial design phase to the wall’s completion would be five to 10 years. \"I think that would be reasonable,\" he speculated.', 'label': 'true', 'scored_sentences': \"''Building a wall'' on the border ''will take literally years. Rick Perry, agog. The logistics of that idea leave many people, including former Texas Gov.\", 'justification_sentences': ['Perry said: \"Building a wall\" on the U.S.-Mexico border \"will take literally years.\"', 'If Trump has a fast-track plan to plan the wall, purchase required land, complete needed studies and erect the wall in a year or less, it’s not public.', 'Meantime, engineering experts agree the wall would most likely take years to complete.', 'Keep in mind, too, it took more than six years to build roughly 700 miles of fence and barriers along the roughly 2,000-mile U.S.-Mexico border.', 'Click here formore on the six PolitiFact ratings and how we select facts to check.']}\n"
     ]
    }
   ],
   "source": [
    "sa_args = get_model_args()\n",
    "\n",
    "# sa_args.dataset_name = 'liar'\n",
    "# sa_args.sentences_path = \"/Users/jolly/PycharmProjects/COPENLU/FilteredData/sup_sccores_liar/results_serialized_test_filtered.jsonl\"\n",
    "# sa_args.dataset_path = \"/Users/jolly/PycharmProjects/COPENLU/liar_data/ruling_oracles_test.tsv\"\n",
    "# sa_args.top_n = 3\n",
    "\n",
    "# dataset = get_dataset(sa_args)\n",
    "# pipeline_inp = [line for line in open('/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/liar/liar_sup_test.txt', 'r')] #sa_inp (pipeline-inp)+ '\\t' +sa_out\n",
    "# pipeline_out = [line for line in open('/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/liar/liar_sup_test_pegasus.txt', 'r')]\n",
    "\n",
    "sa_args.dataset_name = 'pubhealth'\n",
    "sa_args.sentences_path = \"/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/pubhealth/results_serialized_test_filtered.jsonl\"\n",
    "sa_args.dataset_path = \"/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/pubhealth/test.tsv\"\n",
    "dataset = get_dataset(sa_args)\n",
    "pipeline_inp = [line for line in open('/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/pubhealth/pub_sup_test.txt', 'r')] #sa_inp (pipeline-inp)+ '\\t' +sa_out\n",
    "pipeline_out = [line for line in open('/Users/jolly/PycharmProjects/COPENLU/data-used-for-HE-prep/pubhealth/pub_sup_test_pegasus.txt', 'r')]\n",
    "sa_args.top_n = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1248/1248 [00:07<00:00, 178.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores)\n",
      "{'rouge1': AggregateScore(low=Score(precision=0.395032798626512, recall=0.3235236098291057, fmeasure=0.3291102821595914), mid=Score(precision=0.40441688548875565, recall=0.3311518163318101, fmeasure=0.33554521250787134), high=Score(precision=0.41431664625001186, recall=0.3384736012620341, fmeasure=0.34200122273969036)),\n",
      " 'rouge2': AggregateScore(low=Score(precision=0.15555399988337748, recall=0.12075426635918562, fmeasure=0.12497195824768835), mid=Score(precision=0.16438397471201333, recall=0.12722761820315598, fmeasure=0.1311786919246722), high=Score(precision=0.17286865800918366, recall=0.13371411422600787, fmeasure=0.13743479698852543)),\n",
      " 'rougeLsum': AggregateScore(low=Score(precision=0.34763133050842415, recall=0.28244710678224605, fmeasure=0.2881896767842884), mid=Score(precision=0.3571951596940012, recall=0.2890474783509336, fmeasure=0.29407225188022723), high=Score(precision=0.3667481324105058, recall=0.29626276989477757, fmeasure=0.3001459355001099))}\n",
      "rouge1 P: 40.44 \t CI(39.50-41.43) \t  R: 33.12 \t CI(32.35-33.85) \t  F1: 33.55 \t CI(32.91-34.20)\n",
      "rouge2 P: 16.44 \t CI(15.56-17.29) \t  R: 12.72 \t CI(12.08-13.37) \t  F1: 13.12 \t CI(12.50-13.74)\n",
      "rougeLsum P: 35.72 \t CI(34.76-36.67) \t  R: 28.90 \t CI(28.24-29.63) \t  F1: 29.41 \t CI(28.82-30.01)\n",
      "Processed:  1248\n"
     ]
    }
   ],
   "source": [
    "score_names = ['rouge1', 'rouge2', 'rougeLsum']\n",
    "scorer = rouge_scorer.RougeScorer(score_names, use_stemmer=True)\n",
    "scores = []\n",
    "\n",
    "processed_samples = 0\n",
    "for org in tqdm(dataset):\n",
    "    processed_samples+=1\n",
    "    #print(org['scored_sentences'])\n",
    "    score1 = scorer.score(prediction='\\n'.join(sent_tokenize(org['scored_sentences'])),\n",
    "                              target='\\n'.join(org['justification_sentences']))\n",
    "    scores.append(score1)\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Scores)\")\n",
    "aggregate_print_rouges(score_names, scores)\n",
    "print(\"Processed: \", processed_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep for Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(420)# - To save data for task 1\n",
    "final_data = []\n",
    "\n",
    "for org, inp, out in zip(dataset, pipeline_inp, pipeline_out):\n",
    "    \n",
    "    claim = org[\"statement\"]\n",
    "    veracity_label = org[\"label\"]\n",
    "    pipe_inp = inp.split(\"\\t\")[0]\n",
    "    pipe_out = out\n",
    "    \n",
    "    line_data = {\n",
    "        \"claim\": claim,\n",
    "        \"label\": veracity_label,\n",
    "        \"pipe_inp\": pipe_inp,\n",
    "        \"pipe_out\": pipe_out}\n",
    "    \n",
    "    final_data.append(line_data)\n",
    "\n",
    "final_data_40 = []\n",
    "for idx, line in enumerate(random.sample(final_data, 40)):\n",
    "    line[\"id\"] = idx+1\n",
    "    final_data_40.append(line)\n",
    "    \n",
    "json.dump(final_data_40, open(\"he_pub_task1.json\", \"w\"), indent=2)\n",
    "print(len(final_data_40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep for Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save data for task2\n",
    "\n",
    "random.seed(40) \n",
    "\n",
    "final_data = []\n",
    "\n",
    "for org, inp, out in zip(dataset, pipeline_inp, pipeline_out):\n",
    "    \n",
    "    claim = org[\"statement\"]\n",
    "    veracity_label = org[\"label\"]\n",
    "    pipe_inp = inp.split(\"\\t\")[0]\n",
    "    pipe_out = out\n",
    "    \n",
    "    line_data = {\n",
    "        \"claim\": claim,\n",
    "        \"label\": veracity_label,\n",
    "        \"just\": pipe_inp,\n",
    "        \"just_type\": \"pipe_inp\"\n",
    "    }\n",
    "    \n",
    "    final_data.append(line_data)\n",
    "for idx, line in enumerate(random.sample(final_data, 30)):\n",
    "    new_final_data.append(line)\n",
    "print(len(new_final_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_60 = []\n",
    "for idx, line in enumerate(random.sample(new_final_data, 60)):\n",
    "    line[\"id\"] = idx+1\n",
    "    final_data_60.append(line)\n",
    "    \n",
    "json.dump(final_data_60, open(\"he_pub_task2.json\", \"w\"), indent=2)\n",
    "print(len(final_data_60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read SA output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = [line for line in open('../OUTs-Server/to_Pepa_forlabelpred/pub_sup/pub_sup_test.txt')]\n",
    "\n",
    "file2 = [line for line in open('../OUTs-Server/to_Pepa_forlabelpred/pub_sup/pub_sup_test_pp.txt')]\n",
    "\n",
    "file3 = [line for line in open('../OUTs-Server/to_Pepa_forlabelpred/pub_sup/pub_sup_test_pegasus.txt')]\n",
    "\n",
    "for i, j, k, org in zip(file1, file2, file3, dataset):\n",
    "    \n",
    "    print(i.split(\"\\t\")[0] + \"\\n\")\n",
    "    print(j + \"\\n\")\n",
    "    print(k + \"\\n\")\n",
    "    print(\"claim: \", org[\"statement\"])\n",
    "    print(\"label: \", org[\"label\"])\n",
    "    input()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CleanSents using Paraphrase Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import language_tool_python\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "model_name = 'tuner007/pegasus_paraphrase'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "model_sbert = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "num_beams = 10\n",
    "num_return_sequences = 10\n",
    "\n",
    "def get_response(input_text,num_return_sequences,num_beams):\n",
    "    batch = tokenizer([input_text],truncation=True,padding='longest',max_length=60, return_tensors=\"pt\").to(torch_device)\n",
    "    translated = model.generate(**batch, max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text\n",
    "\n",
    "def gramatical_tool(sent):\n",
    "    matches = tool.check(sent)\n",
    "    return language_tool_python.utils.correct(sent, matches)\n",
    "\n",
    "def sentence_level_semantic_scorer_sbert(org, rep):\n",
    "    org_embeds = model_sbert.encode(org)\n",
    "    rep_embeds = model_sbert.encode(rep)\n",
    "    return torch.FloatTensor([util.pytorch_cos_sim(e1, e2) for e1, e2 in zip(org_embeds, rep_embeds)])\n",
    "\n",
    "def post_process(text):\n",
    "    valid_sents = []\n",
    "    for i in sent_tokenize(text):\n",
    "        i = gramatical_tool(i)\n",
    "        doc = nlp(i)\n",
    "        verbs = [token.text for token in doc if token.pos_ in ['VERB', 'AUX']]\n",
    "        if len(verbs)>0:\n",
    "            valid_sents.append(i)\n",
    "            \n",
    "    return \" \".join(valid_sents)\n",
    "\n",
    "def pegasus(text):\n",
    "    temp = []\n",
    "    for i in sent_tokenize(text):\n",
    "        if len(i.split(\" \")) == 1:\n",
    "            temp.append(i)\n",
    "            continue\n",
    "        else:\n",
    "            all_responses = get_response(i, num_return_sequences, num_beams)\n",
    "            temp_str = ''\n",
    "            sim = sentence_level_semantic_scorer_sbert(all_responses, [i]*10)\n",
    "            max_sim_rep = all_responses[torch.argmax(sim)]    \n",
    "            temp.append(max_sim_rep)\n",
    "    \n",
    "    return(\" \".join(temp))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = json.load(open(\"he_data_liar.json\"))\n",
    "for line in tqdm(outs):\n",
    "    line['sa_pp'] = post_process(line[\"sa_out\"])\n",
    "    line['sa_pegasus'] = pegasus(line[\"sa_pp\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json.dump(outs, open('new_postprocess_liar.json', 'w+'))\n",
    "import json\n",
    "outs = json.load(open(\"new_postprocess_liar.json\"))\n",
    "for line in outs:\n",
    "    print(\"sa_inp\", line[\"sa_inp\"])\n",
    "    print(\"-----------\")\n",
    "    print(\"sa_out\", line[\"sa_out\"])\n",
    "    print(\"-----------\")\n",
    "    print(\"sa_pp\", line[\"sa_pp\"])\n",
    "    print(\"-----------\")\n",
    "    print(\"sa_pegasus\", line[\"sa_pegasus\"])\n",
    "    print(\"-----------\")\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JustFact: Generating fact-checking explainations in unsupervised settings\n",
    "\n",
    "Pipeline - Sentences from RC's --> SA --> post-processing to remove grammatical errors --> Pegasus to make it more consise\n",
    "\n",
    "Human Eval - Using two justifications - pipeline inp (SA inp) & pipeline out (pegasus out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scorer_batch(sentences):\n",
    "    #Gpt for fluency\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tensor_input = {k: v.to(\"cpu\") for k,v in tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').items()}\n",
    "\n",
    "\n",
    "    lm_labels = tensor_input[\"input_ids\"].detach().clone()\n",
    "    lm_labels[lm_labels[:, :] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = model(input_ids=tensor_input[\"input_ids\"],\n",
    "                attention_mask= tensor_input[\"attention_mask\"],\n",
    "                return_dict=True)\n",
    "\n",
    "    lm_logits = outputs.logits\n",
    "    shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "    shift_labels = lm_labels[..., 1:].contiguous()\n",
    "    \n",
    "    print([tokenizer._convert_id_to_token(i) for i in shift_labels.tolist()[0]])\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='none')  # give CE loss at each word generation step\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    \n",
    "    log_prob_sum = loss.reshape(-1, shift_labels.shape[-1]) #.sum(dim=1)\n",
    "    log_prob_sum1 = torch.exp(-loss.reshape(-1, shift_labels.shape[-1])) #.sum(dim=1)\n",
    "    len_sum = tensor_input[\"attention_mask\"][..., 1:].contiguous() #.sum(dim=1)\n",
    "    \n",
    "    #prob_products_per_sample = torch.exp(-1 * (log_prob_sum/len_sum)).cpu()\n",
    "\n",
    "    print(log_prob_sum)\n",
    "    #print(log_prob_sum1)\n",
    "    print(len_sum)\n",
    "    \n",
    "    print(log_prob_sum.sum(dim=1))\n",
    "    #print(log_prob_sum1)\n",
    "    print(len_sum.sum(dim=1))\n",
    "    print(log_prob_sum.sum(dim=1) / len_sum.sum(dim=1))\n",
    "    \n",
    "    print(\"\\nFinal:\", 100 * torch.exp(- 1 * (log_prob_sum.sum(dim=1) / len_sum.sum(dim=1))))\n",
    "    \n",
    "    #return (prob_products_per_sample * 100)\n",
    "    \n",
    "sents = ['I bought bananas, apples, and orange juice from the supermarket.']\n",
    "scorer_batch(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep SA input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(sent):\n",
    "    sent = sent.replace(\"’\", \"'\")\n",
    "    sent = sent.replace(\"‘\", \"`\")\n",
    "    sent = sent.replace('\"', \"''\")\n",
    "    sent = sent.replace(\"—\", \"--\")\n",
    "    sent = sent.replace(\"…\", \"...\")\n",
    "    sent = sent.replace(\"–\", \"--\")\n",
    "\n",
    "    return sent\n",
    "\n",
    "def get_dataset(scored_sentences_path, dataset_path, dataset_name, top_n, parser):\n",
    "\n",
    "    if dataset_name == 'liar_plus':\n",
    "        df = pd.read_csv(dataset_path, sep='\\t', index_col=0)\n",
    "        df = df.dropna()\n",
    "        columns = ['dummy', 'id', 'statement', 'justification',\n",
    "               'ruling_without_summary', 'label', 'just_tokenized',\n",
    "               'ruling_tokenized', 'statement_tokenized', 'oracle_ids']\n",
    "        print(df.columns)\n",
    "        print(columns)\n",
    "        df.columns = columns\n",
    "        \n",
    "    elif dataset_name == 'pub_health':\n",
    "        df = pd.read_csv(dataset_path, sep='\\t', index_col=0)\n",
    "        df = df.dropna()\n",
    "        \n",
    "        columns = ['claim_id', 'claim', 'date_published', 'explanation',\n",
    "                   'fact_checkers', 'main_text', 'sources', 'label', 'subjects']\n",
    "        \n",
    "        if len(df.columns) == 10:\n",
    "            columns = ['dummy'] + columns\n",
    "        \n",
    "        df.columns = columns\n",
    "        \n",
    "    scored_sentences = [json.loads(line) for line in open(scored_sentences_path)]\n",
    "    scored_sentences = {item[\"id\"]: sorted(item['sentence_scores'], key=lambda x: x[1], reverse=True)[:top_n] for item in scored_sentences}\n",
    "    \n",
    "    \n",
    "    inp_scored_sentences = {}\n",
    "    for k, v in scored_sentences.items():\n",
    "        \n",
    "        temp = []\n",
    "        for sent in v:\n",
    "            temp.append(sent[0])\n",
    "        inp_scored_sentences[k] = clean_str(\" \".join(temp))\n",
    "\n",
    "    scored_sentences = inp_scored_sentences\n",
    "    \n",
    "    \n",
    "    if dataset_name == 'liar_plus':\n",
    "        \n",
    "        df['scored_sentences'] = df.apply(lambda x: scored_sentences.get(x['id'], None), axis=1)\n",
    "        df = df[df['scored_sentences'] != None]\n",
    "        df['justification_sentences'] = df.apply(lambda x: sent_tokenize(x['justification']), axis=1)\n",
    "        df = df[['id', 'statement', 'justification', 'label', 'scored_sentences',\n",
    "             'justification_sentences']]\n",
    "        \n",
    "    elif dataset_name == 'pub_health':\n",
    "        df['claim_id'] = df['claim_id'].astype('str')\n",
    "        df['scored_sentences'] = df.apply(lambda x: scored_sentences.get(x['claim_id'], None), axis=1)\n",
    "        df = df[df['scored_sentences'] != None]\n",
    "        df['justification_sentences'] = df.apply(lambda x: sent_tokenize(x['explanation']), axis=1)\n",
    "        df = df[['claim_id', 'claim', 'explanation', 'label', 'scored_sentences',\n",
    "             'justification_sentences']]\n",
    "        \n",
    "        \n",
    "    dataset = [row.to_dict() for i, row in df.iterrows()]\n",
    "    new_dataset = []\n",
    "    if dataset_name == 'liar_plus':\n",
    "        for i in dataset:\n",
    "            if i[\"scored_sentences\"] is None or i[\"id\"] == '2001.json': #Sentence in Liarplus is too long:\n",
    "                continue\n",
    "            else:\n",
    "                new_dataset.append(i)\n",
    "    elif dataset_name == 'pub_health':\n",
    "        for i in dataset:\n",
    "        \n",
    "            if i[\"scored_sentences\"] is None or i[\"scored_sentences\"] == None:\n",
    "                continue\n",
    "            else:\n",
    "                new_dataset.append(i)\n",
    "    \n",
    "\n",
    "    print(f'Size of dataset: {len(dataset)}')\n",
    "    print(f'Size of new dataset: {len(new_dataset)}')\n",
    "    print('Sample: ', dataset[0])\n",
    "    if len(new_dataset)!=0:\n",
    "        print('Sample: ', new_dataset[0])\n",
    "\n",
    "    return \n",
    "\n",
    "scored_sentences_path = \"../../DATA-COPE-Project-DIKUServer/unsup_scores_liar/sentence_scores_val.jsonl\" #Each line is a json\n",
    "scored_sentences_path1 = \"../../DATA-COPE-Project-DIKUServer/unsup_scores_pubhealth/sentence_scores_test.jsonl\"\n",
    "\n",
    "dataset_path = \"../../liar_data/ruling_oracles_val.tsv\"\n",
    "dataset_path1 = \"../../DATA-COPE-Project-DIKUServer/PUBHEALTH/test.tsv\"\n",
    "\n",
    "get_dataset(scored_sentences_path1, dataset_path1, 'pub_health', 6, parser)\n",
    "#get_dataset(scored_sentences_path, dataset_path, 'liar_plus', 6, parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
